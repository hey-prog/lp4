{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e60d97-88fb-4f15-9787-7bdbfc4f4455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LP-IV Assignment: Continuous Bag of Words (CBOW)\n",
    "# Works for any input text (Document 1 / 2 / 3)\n",
    "# ============================================================\n",
    "\n",
    "# a. Data Preparation\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ðŸ“„ STEP 1: Add or Load Your Document Text\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Option 1: Paste text directly here\n",
    "data = \"\"\"\n",
    "PASTE YOUR DOCUMENT TEXT HERE\n",
    "\"\"\"\n",
    "\n",
    "# --- OR ---\n",
    "# Option 2: Load from .txt file\n",
    "# with open(\"LP-IV-datasets/CBOW/document1.txt\", 'r', encoding='utf-8') as f:\n",
    "#     data = f.read()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Clean and preprocess the text\n",
    "# ------------------------------------------------------------\n",
    "sentences = data.split('.')\n",
    "clean_sent = []\n",
    "for sentence in sentences:\n",
    "    if sentence.strip() == \"\":\n",
    "        continue\n",
    "    sentence = re.sub('[^A-Za-z0-9]+', ' ', sentence)\n",
    "    sentence = re.sub(r'(?:^| )\\w (?:$| )', ' ', sentence).strip()\n",
    "    sentence = sentence.lower()\n",
    "    clean_sent.append(sentence)\n",
    "\n",
    "print(\"Total sentences:\", len(clean_sent))\n",
    "print(\"Sample sentence:\", clean_sent[0] if clean_sent else \"No valid text!\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# b. Generate Training Data\n",
    "# ------------------------------------------------------------\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_sent)\n",
    "sequences = tokenizer.texts_to_sequences(clean_sent)\n",
    "\n",
    "index_to_word = {}\n",
    "word_to_index = {}\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    words = clean_sent[i].split()\n",
    "    for j, value in enumerate(sequence):\n",
    "        index_to_word[value] = words[j]\n",
    "        word_to_index[words[j]] = value\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "emb_size = 10\n",
    "context_size = 2\n",
    "\n",
    "contexts, targets = [], []\n",
    "\n",
    "for sequence in sequences:\n",
    "    for i in range(context_size, len(sequence) - context_size):\n",
    "        target = sequence[i]\n",
    "        context = [\n",
    "            sequence[i - 2], sequence[i - 1],\n",
    "            sequence[i + 1], sequence[i + 2]\n",
    "        ]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "\n",
    "X = np.array(contexts)\n",
    "Y = np.array(targets)\n",
    "\n",
    "print(f\"Total training samples: {X.shape[0]} | Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# c. Train CBOW Model\n",
    "# ------------------------------------------------------------\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=emb_size, input_length=2 * context_size),\n",
    "    Lambda(lambda x: tf.reduce_mean(x, axis=1)),   # averages context vectors\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, Y, epochs=60, verbose=1)\n",
    "# --- Alternative (for faster testing): epochs=30 or batch_size=64 ---\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# d. Output and Visualization\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.lineplot(x=range(len(history.history['loss'])),\n",
    "             y=history.history['loss'], label='Training Loss')\n",
    "plt.title(\"CBOW Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print vocabulary and sample\n",
    "print(\"\\nðŸ“š Vocabulary Sample:\")\n",
    "print(list(index_to_word.values())[:20])\n",
    "\n",
    "# Visualize embeddings using PCA\n",
    "embeddings = model.get_weights()[0]\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Test predictions for sample contexts\n",
    "test_phrases = [\n",
    "    \"the speed of\", \n",
    "    \"transmission is an\", \n",
    "    \"influenza has a\",\n",
    "    \"the serial interval\"\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ” Predicted words for sample contexts:\")\n",
    "for phrase in test_phrases:\n",
    "    words = phrase.split()\n",
    "    x_test = [word_to_index.get(i, 0) for i in words]\n",
    "    x_test = np.array([x_test])\n",
    "    pred = model.predict(x_test, verbose=0)\n",
    "    pred_word = index_to_word.get(np.argmax(pred[0]), \"unknown\")\n",
    "    print(f\"Context: {words}  -->  Predicted center word: {pred_word}\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"cbow_model_generic.keras\")\n",
    "\n",
    "print(\"\\n CBOW Model training completed successfully for this document.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
