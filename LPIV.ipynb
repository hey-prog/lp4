{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b85c0-dee6-4623-b2cb-be06a24a1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                 LP-IV MASTER CODE FILE\n",
    "# ============================================================\n",
    "# Contains all finalized template codes for:\n",
    "# 1️⃣ Feedforward Neural Network (FNN)\n",
    "# 2️⃣ Convolutional Neural Network (CNN)\n",
    "# 3️⃣ Autoencoder (Anomaly Detection)\n",
    "# 4️⃣ Continuous Bag of Words (CBOW)\n",
    "# 5️⃣ Object Detection (Transfer Learning)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1️⃣ FEEDFORWARD NEURAL NETWORK (FNN)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n=== Running FNN ===\")\n",
    "\n",
    "train = pd.read_csv('LP-IV-datasets/CIFR(Ass2&3)/train_data.csv')\n",
    "test  = pd.read_csv('LP-IV-datasets/CIFR(Ass2&3)/test_data.csv')\n",
    "\n",
    "X_train = train.drop('label', axis=1).values / 255.0\n",
    "y_train = train['label'].values\n",
    "X_test  = test.drop('label', axis=1).values / 255.0\n",
    "y_test  = test['label'].values\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=(3072,)),\n",
    "    # Alternative (Adam): add Dropout and deeper layers\n",
    "    # layers.Dropout(0.3),\n",
    "    # layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer_choice = keras.optimizers.SGD()\n",
    "# Alternative:\n",
    "# optimizer_choice = keras.optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=optimizer_choice,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=11, batch_size=128, validation_split=0.1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Training & Validation Loss (FNN)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Training & Validation Accuracy (FNN)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2️⃣ CONVOLUTIONAL NEURAL NETWORK (CNN)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n=== Running CNN ===\")\n",
    "\n",
    "train = pd.read_csv('LP-IV-datasets/CIFR(Ass2&3)/train_data.csv', nrows=10000)\n",
    "test  = pd.read_csv('LP-IV-datasets/CIFR(Ass2&3)/test_data.csv',  nrows=2000)\n",
    "\n",
    "X_train = train.drop('label', axis=1).values.reshape(-1, 32, 32, 3).astype('float32') / 255.0\n",
    "y_train = train['label'].values\n",
    "X_test  = test.drop('label', axis=1).values.reshape(-1, 32, 32, 3).astype('float32') / 255.0\n",
    "y_test  = test['label'].values\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Alternative (Deep CNN): Use multi-block model from previous version.\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=11, batch_size=128, validation_split=0.1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Training & Validation Loss (CNN)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Training & Validation Accuracy (CNN)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3️⃣ AUTOENCODER (ANOMALY DETECTION)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\n=== Running Autoencoder ===\")\n",
    "\n",
    "data = pd.read_csv(\"LP-IV-datasets/CreditCard/credit_card_data.csv\")\n",
    "X = data.drop('Class', axis=1).values\n",
    "y = data['Class'].values\n",
    "\n",
    "X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0) + 1e-6)\n",
    "X_train = X[y == 0]\n",
    "X_test = X\n",
    "y_test = y\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "encoded = layers.Dense(64, activation='relu')(input_layer)\n",
    "encoded = layers.Dense(32, activation='relu')(encoded)\n",
    "encoded = layers.Dense(16, activation='relu')(encoded)\n",
    "decoded = layers.Dense(32, activation='relu')(encoded)\n",
    "decoded = layers.Dense(64, activation='relu')(decoded)\n",
    "decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = models.Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train, epochs=20, batch_size=256, validation_split=0.1, shuffle=True)\n",
    "\n",
    "reconstructions = autoencoder.predict(X_test)\n",
    "mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)\n",
    "threshold = np.percentile(mse, 95)\n",
    "y_pred = (mse > threshold).astype(int)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Autoencoder Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4️⃣ CONTINUOUS BAG OF WORDS (CBOW)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n=== Running CBOW ===\")\n",
    "\n",
    "data = \"\"\"PASTE YOUR DOCUMENT TEXT HERE\"\"\"\n",
    "\n",
    "sentences = data.split('.')\n",
    "clean_sent = []\n",
    "for sentence in sentences:\n",
    "    if sentence.strip() == \"\":\n",
    "        continue\n",
    "    sentence = re.sub('[^A-Za-z0-9]+', ' ', sentence)\n",
    "    sentence = re.sub(r'(?:^| )\\w (?:$| )', ' ', sentence).strip()\n",
    "    sentence = sentence.lower()\n",
    "    clean_sent.append(sentence)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_sent)\n",
    "sequences = tokenizer.texts_to_sequences(clean_sent)\n",
    "index_to_word, word_to_index = {}, {}\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    words = clean_sent[i].split()\n",
    "    for j, value in enumerate(sequence):\n",
    "        index_to_word[value] = words[j]\n",
    "        word_to_index[words[j]] = value\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "context_size, emb_size = 2, 10\n",
    "contexts, targets = [], []\n",
    "\n",
    "for sequence in sequences:\n",
    "    for i in range(context_size, len(sequence) - context_size):\n",
    "        target = sequence[i]\n",
    "        context = [sequence[i - 2], sequence[i - 1], sequence[i + 1], sequence[i + 2]]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "\n",
    "X, Y = np.array(contexts), np.array(targets)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=emb_size, input_length=2 * context_size),\n",
    "    Lambda(lambda x: tf.reduce_mean(x, axis=1)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X, Y, epochs=60, verbose=1)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.lineplot(x=range(len(history.history['loss'])), y=history.history['loss'], label='Training Loss')\n",
    "plt.title(\"CBOW Model Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "embeddings = model.get_weights()[0]\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(embeddings)\n",
    "\n",
    "print(\"\\nSample Vocabulary:\", list(index_to_word.values())[:20])\n",
    "print(\"✅ CBOW Training Complete.\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5️⃣ OBJECT DETECTION (TRANSFER LEARNING)\n",
    "# ============================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n=== Running Object Detection (Transfer Learning) ===\")\n",
    "\n",
    "weights_path = \"Object Detection(Ass6)/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "train_dir = \"Object Detection(Ass6)/caltech-101-img/\"\n",
    "\n",
    "base_model = VGG16(weights=weights_path, include_top=False, input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2, rotation_range=20,\n",
    "                             width_shift_range=0.1, height_shift_range=0.1,\n",
    "                             zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(train_dir, target_size=(224,224), batch_size=32,\n",
    "                                        class_mode='categorical', subset='training')\n",
    "val_gen = datagen.flow_from_directory(train_dir, target_size=(224,224), batch_size=32,\n",
    "                                      class_mode='categorical', subset='validation')\n",
    "\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(len(train_gen.class_indices), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_gen, epochs=10, validation_data=val_gen)\n",
    "\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "fine_tune_history = model.fit(train_gen, epochs=5, validation_data=val_gen)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'] + fine_tune_history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'] + fine_tune_history.history['val_loss'], label='Val Loss')\n",
    "plt.title(\"Training & Validation Loss (Object Detection)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'] + fine_tune_history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'] + fine_tune_history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title(\"Training & Validation Accuracy (Object Detection)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "model.save(\"object_detection_vgg16.keras\")\n",
    "print(\"\\n✅ Object Detection Model Training Complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
